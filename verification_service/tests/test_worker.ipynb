{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-25T04:17:58.163418Z",
     "start_time": "2024-06-25T04:17:56.297746Z"
    }
   },
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import shutil\n",
    "import uuid\n",
    "from verification_service.worker.jobs import Supervisor, Worker\n",
    "from verification_service.storage.database import MongoDbConnector\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "from functools import partial\n",
    "from verification_service import unique_id\n",
    "\n",
    "\n",
    "_outs = './test_outputs'\n",
    "if os.path.exists(_outs):\n",
    "    shutil.rmtree(_outs)\n",
    "\n",
    "\n",
    "def jobid(): return str(uuid.uuid4())\n",
    "\n",
    "\n",
    "load_dotenv(\"../verification_service/.env\")\n",
    "uri = os.getenv(\"MONGO_DB_URI\")\n",
    "omex_source_dir = './examples/sbml-core'\n",
    "omex_name = 'Elowitz-Nature-2000-Repressilator.omex'\n",
    "omex_fp = os.path.join(omex_source_dir, omex_name)\n",
    "out_dir = './test_outputs'\n",
    "simulators = ['amici', 'copasi', 'tellurium']\n",
    "spec_name = 'cI mRNA'\n",
    "job_id = jobid()\n",
    "\n",
    "db_connector = MongoDbConnector(connection_uri=uri, database_id=\"service_requests\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoldyn is not properly installed in this environment and thus its process implementation cannot be registered. Please consult smoldyn documentation.\n",
      "Cannot register SimpleNeuron. Error:\n",
      "**\n",
      "No module named 'pyneuroml'\n",
      "**\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cascading_load_arrows' from 'verification_service.data_model.worker' (/Users/alexanderpatrie/Desktop/repos/verification-service/verification_service/data_model/worker.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mshutil\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01muuid\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mverification_service\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mworker\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjobs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Supervisor, Worker\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mverification_service\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstorage\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatabase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MongoDbConnector\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpymongo\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmongo_client\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MongoClient\n",
      "File \u001B[0;32m~/Desktop/repos/verification-service/verification_service/worker/jobs.py:28\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mverification_service\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_model\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mshared\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseClass, MultipleConnectorError\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mverification_service\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstorage\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatabase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MongoDbConnector\n\u001B[0;32m---> 28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mverification_service\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_model\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mworker\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m UtcComparison, SimulationError, UtcSpeciesComparison, cascading_load_arrows\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mverification_service\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_sbml_species_names, get_sbml_model_file_from_archive, read_report_outputs\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mverification_service\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mworker\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moutput_data\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m generate_biosimulator_utc_outputs, _get_output_stack\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'cascading_load_arrows' from 'verification_service.data_model.worker' (/Users/alexanderpatrie/Desktop/repos/verification-service/verification_service/data_model/worker.py)"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "supervisor = Supervisor(db_connector=db_connector)",
   "id": "fc2f44df59a7156e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "supervisor.jobs",
   "id": "d7cb48762b805377",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# re-create loop here\n",
    "\n",
    "import asyncio\n",
    "from verification_service import load_arrows\n",
    "\n",
    "\n",
    "db_connector = MongoDbConnector(connection_uri=uri, database_id=\"service_requests\")\n",
    "supervisor = Supervisor(db_connector=db_connector)\n",
    "jobs_to_complete = []\n",
    "        \n",
    "\n",
    "async def fetch_jobs(supervisor: Supervisor, max_retries=5, delay=5):\n",
    "    pending_jobs = [job for job in supervisor.db_connector.db['pending_jobs'].find()]\n",
    "    async def _run_check():\n",
    "        if len(pending_jobs):\n",
    "            print('There are pending jobs')\n",
    "            result = await check_jobs(supervisor)\n",
    "            print('There are no pending jobs')\n",
    "            return result\n",
    "        else:\n",
    "            return None \n",
    "            \n",
    "    n_retries = 0\n",
    "    run = True\n",
    "    while run:\n",
    "        check = await _run_check()\n",
    "        if check is None:\n",
    "            n_retries += 1\n",
    "            await asyncio.sleep(delay)\n",
    "        if n_retries == max_retries:\n",
    "            run = False\n",
    "        else:\n",
    "            continue \n",
    "    return 0 \n",
    "        \n",
    "        \n",
    "async def check_jobs(supervisor, max_retries=5, delay=5) -> int:\n",
    "    job_queue = supervisor.pending_jobs\n",
    "    n_tries = 0\n",
    "    while True:\n",
    "        # count tries\n",
    "        n_tries += 1\n",
    "        if n_tries > 1:\n",
    "            await asyncio.sleep(delay)\n",
    "        elif n_tries == max_retries + 1:\n",
    "            print(f'Max retries {max_retries} reached!')\n",
    "            break\n",
    "        else:\n",
    "            if len(job_queue):\n",
    "                print('There are pending jobs.')\n",
    "                for i, job in enumerate(job_queue):\n",
    "                    # get the next job in the queue based on the preferred_queue_index\n",
    "                    job_doc = supervisor.pending_jobs.pop(supervisor.preferred_queue_index)\n",
    "                    job_comparison_id = job_doc['comparison_id']\n",
    "                    unique_id_query = {'comparison_id': job_comparison_id}\n",
    "                    in_progress_job = supervisor.db_connector.db.in_progress_jobs.find_one(unique_id_query) or None\n",
    "                    \n",
    "                    _job_exists = partial(supervisor._job_exists, comparison_id=job_comparison_id)\n",
    "                    if not _job_exists(collection_name='in_progress_jobs'):\n",
    "                        print(f\"In progress job does not yet exist for {job_comparison_id}\")\n",
    "                        in_progress_job_id = unique_id()\n",
    "                        worker_id = unique_id()\n",
    "                        id_kwargs = ['job_id', 'worker_id']\n",
    "                        in_prog_kwargs = dict(zip(\n",
    "                            id_kwargs,\n",
    "                            list(map(lambda k: unique_id(), id_kwargs))\n",
    "                        ))\n",
    "                        in_prog_kwargs['comparison_id'] = job_comparison_id\n",
    "                        \n",
    "                        supervisor.db_connector.insert_in_progress_job(**in_prog_kwargs)\n",
    "                        print(f\"Successfully created new progress job for {job_comparison_id}\")\n",
    "                        # await supervisor.async_refresh_jobs()\n",
    "                        \n",
    "                    if not _job_exists(collection_name='completed_jobs'):\n",
    "                        print(f\"Completed job does not yet exist for {job_comparison_id}\")\n",
    "                        # pop in-progress job from internal queue and use it parameterize the worker\n",
    "                        in_prog_id = [job for job in db_connector.in_progress_jobs.find()].pop(supervisor.preferred_queue_index)['job_id']\n",
    "                        \n",
    "                        # double-check and verify doc\n",
    "                        in_progress_doc = supervisor.db_connector.db.in_progress_jobs.find_one({'job_id': in_prog_id})\n",
    "                        \n",
    "                        # generate new worker\n",
    "                        workers_id = in_progress_doc['worker_id']\n",
    "                        worker = supervisor.call_worker(job_params=job_doc, worker_id=workers_id)\n",
    "                        \n",
    "                        # add the worker to the list of workers (for threadsafety)\n",
    "                        supervisor.workers.insert(supervisor.preferred_queue_index, worker.worker_id)\n",
    "                        \n",
    "                        # the worker returns the job result to the supervisor who saves it as part of a new completed job in the database\n",
    "                        completed_doc = supervisor.db_connector.insert_completed_job(job_id=unique_id(), comparison_id=job_comparison_id, results=worker.job_result)\n",
    "                        \n",
    "                        # release the worker from being busy and refresh jobs\n",
    "                        supervisor.workers.pop(supervisor.preferred_queue_index)\n",
    "                        print(f\"Successfully created new completed job for {job_comparison_id}\")\n",
    "                        # await supervisor.async_refresh_jobs()\n",
    "                \n",
    "                    # remove the job from queue\n",
    "                    supervisor.pending_jobs.pop(i)\n",
    "        # sleep\n",
    "        print(f'Sleeping for {delay} seconds...')\n",
    "        await load_arrows(delay)\n",
    "                 \n",
    "    return 0"
   ],
   "id": "a131e38d9a2b3c77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "result = await check_jobs(supervisor, max_retries=5, delay=3)",
   "id": "f0af2e2a5dc958c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "supervisor.pending_jobs",
   "id": "61c8aa74aa7752c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "supervisor.check_jobs()",
   "id": "f005de3bf6385eaa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. get an unassigned pending job by id\n",
    "job_id = supervisor.jobs['pending_jobs'].pop(0)"
   ],
   "id": "bc52d4fbe3785566",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. fetch the respective document/job\n",
    "job_doc = supervisor.db_connector.db.pending_jobs.find_one({'job_id': job_id})"
   ],
   "id": "ee97da80e23bffcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "job_doc",
   "id": "8345463027d1c5ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "job_params = job_doc.copy()",
   "id": "cc5091bcd294c5bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os \n",
    "from verification_service import unique_id\n",
    "\n",
    "os.path.exists(job_params['omex_path'])"
   ],
   "id": "603082a45b57c709",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Create a new in process job for the pending job we just picked up\n",
    "worker_id = unique_id()\n",
    "in_progress_job_id = unique_id()\n",
    "in_progress_doc = supervisor.db_connector.insert_in_progress_job(\n",
    "    job_id=in_progress_job_id,\n",
    "    worker_id=worker_id,\n",
    "    comparison_id=job_doc['comparison_id'],\n",
    ")\n",
    "\n",
    "in_progress_doc"
   ],
   "id": "e804aea0cffe0164",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Call the worker who will automatically process the job\n",
    "worker = Worker(job_params=job_params)"
   ],
   "id": "50521f80310bf489",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Get the result from the worker and insert the new completed job for that comparison_id\n",
    "from verification_service import unique_id\n",
    "\n",
    "comparison_id = job_doc['comparison_id']\n",
    "\n",
    "completed_doc = supervisor.db_connector.insert_completed_job(\n",
    "    job_id=unique_id(),\n",
    "    comparison_id=comparison_id,\n",
    "    results=worker.job_result\n",
    ")"
   ],
   "id": "2a7a505a7a6ed538",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "completed_doc",
   "id": "2d457f7201db74f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test `Supervisor.initialize()`",
   "id": "fb9b7b99000cdd75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1df89daef32eb532",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "\n",
    "# case: uncompleted/pending jobs exist\n",
    "jobs_to_complete = pending\n",
    "if len(pending):\n",
    "    in_progress_jobs = supervisor.jobs['in_progress_jobs']\n",
    "    preferred_queue_index = supervisor.preferred_queue_index  # TODO: How can we make this more robust/dyn\n",
    "    "
   ],
   "id": "d394fb091f393652",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "supervisor.get_jobs()",
   "id": "96042befc59a466c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c85b3e7caa2ace2a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
